{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2df169d8-4c37-4450-97b5-6cc781a2ae4a",
   "metadata": {},
   "source": [
    "# DEEE725 Speech Signal Processing Lab\n",
    "### 2023 Spring, Kyungpook National University \n",
    "### Instructor: Gil-Jin Jang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09645527-0887-405f-9a9a-05da9cb45270",
   "metadata": {},
   "source": [
    "# Project 1 Isolated digit recognition in noisy environments\n",
    "\n",
    "- Assigned: 2023/04/21\n",
    "- Due: 2023/05/04\n",
    "- Required dataset: \n",
    "    1. [training data](lab05.pdf)\n",
    "    1. [validation data](lab05.md)\n",
    "    1. [test data](lab05.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f8c951-5371-4e28-af69-2854658a82e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "# import packages, define analysis parameters and draw parameters, audio file preparation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9515d0d3-8af0-47b1-bba5-aedbf3a1f4db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import necessary pacakages\n",
    "# strange issue: keep the import order to prevent matplotlib error\n",
    "#  import matplotlib -> librosa -> pyplot -> librosa.display\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import librosa\n",
    "from matplotlib import pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "#from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from scipy.fft import fftshift\n",
    "\n",
    "# display wav files\n",
    "import IPython #라이브러리 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc26acb-6267-4773-8f15-ca4abb077ff4",
   "metadata": {},
   "source": [
    "오디오 파일들의 경로 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "827c344e-09c7-46fe-b841-0d3ad0f05dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add '/' if path is not a null string : 경로 파일 만드는거\n",
    "def addpath(path, file):\n",
    "    if len(path) == 0: \n",
    "        return file\n",
    "    else:\n",
    "        return path + '/' + file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a593f2-a698-4eb8-b876-9d53cda83ebe",
   "metadata": {},
   "source": [
    "신호 분석과 스펙트럼을 그리기 위한 다음의 parameter 들을 정의한다.\n",
    "입력 파일의 sampling frequency 를 이용하여 shift size 를 sample 수로 정의하기 위해 사용된다.\n",
    "- `Ts`: shift length in seconds, default 0.01 sec = 10 ms. \n",
    "- `Tf`: frame length in seconds, default 0.02 sec = 20 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d94df199-6bc7-4a5f-9c4e-50ee25d7aebe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parameters for signal analysis\n",
    "# Fs = 16000  native sampling frequency (wav file 에 정의된 것) 을 사용하면 필요 없음\n",
    "Ts = 0.01   # 10 ms shift size\n",
    "Tf = 0.02   # 20 ms frame size 타임 샘플링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa43cec4-4631-40db-9f72-1d2827a21db0",
   "metadata": {
    "tags": []
   },
   "source": [
    "spectrum 을 그리기 위한 parameters.\n",
    "- `cmap_plot`: colormap. default value is `pyplot.cm.bone_r` (최소값 흰색, 최대값 검은색 의 gray scale) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9d45a30-c37a-4ddc-9a0a-8f3f3275e202",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parameters for drawing\n",
    "#cmap_plot = plt.cm.bone # default colormap for spectrogram, gray\n",
    "cmap_plot = plt.cm.bone_r # default colormap for spectrogram, gray, reversed\n",
    "#cmap_plot = plt.cm.plasma \n",
    "#cmap_plot = plt.cm.inferno\n",
    "#FIG_SIZE = (15,10)   # obsolete\n",
    "FIG_SIZE = (8,3) #Figure 사이즈"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b3a5e5-0083-47ae-9916-715c6466fb2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### 이전 lab 들에서 정의한 함수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab0a2f81-d319-4137-99ee-2dde50c70ee5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# draw spectrogram\n",
    "from gjdrawspectrogram3 import drawspectrogram3\n",
    "\n",
    "# linear phase FIR filter design from magnitudes of the frequency components\n",
    "from gjfiroverlapadd import getLPHFIRFFT\n",
    "\n",
    "# trapezoidal overlap add for FIR filtering\n",
    "from gjfiroverlapadd import firoverlapadd\n",
    "\n",
    "# save audio in wav format\n",
    "import gjwavfile as wav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbe1916-81e9-4942-bd0b-d105a74bf6d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### load speech and noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699ce62b-6c74-4d57-9cf9-73ee1dca1ae7",
   "metadata": {},
   "source": [
    "오디오 파일이 16 kHz, mono 인지 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "631adcd2-1699-4f7c-b75a-2c4a75d4c40d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1\n",
      "10 2\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros(10)\n",
    "print(len(x), x.ndim)\n",
    "x = np.zeros((10,2))\n",
    "print(len(x), x.ndim) #zero로 채워주는 배열 생성 (쓰지는 않는다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24dd206d-9074-49a1-8373-3a0f9dfd892a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_audio_file(file, defFs, checkMono):\n",
    "    signal, Fs = librosa.load(file, sr=None, mono=False)\n",
    "    if defFs != Fs:\n",
    "        print('sampling rate mismatch, %d != %d for file %s'%(defFs, Fs, file))\n",
    "        return False\n",
    "    elif checkMono == True:\n",
    "        if signal.ndim != 1:\n",
    "            print('not mono file %s, shape='%(file), signal.shape)\n",
    "            return False\n",
    "        return True\n",
    "    elif size(signal) <= 0:\n",
    "        print('wrong audio file %s, shape='%(file), signal.shape)\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def convert_audio_file(file, forceFs, forceMono):\n",
    "    signal, Fs = librosa.load(file, sr=None, mono=False)\n",
    "    changed = False\n",
    "    if forceFs != Fs:\n",
    "        print('sampling rate mismatch, %d != %d for file %s'%(forceFs, Fs, file))\n",
    "        signal, Fs = librosa.load(file, sr=forceFs, mono=False)\n",
    "        changed = True\n",
    "    elif forceMono == True:\n",
    "        if signal.ndim != 1:\n",
    "            print('not mono file %s, shape='%(file), signal.shape)\n",
    "            signal, Fs = librosa.load(file, sr=forceFs, mono=True)\n",
    "            changed = True\n",
    "    elif size(signal) <= 0:\n",
    "        print('wrong audio file %s, shape='%(file), signal.shape)\n",
    "        return False\n",
    "    if changed == True:\n",
    "        wav.writewav(file, Fs, signal, maxval=1.0)\n",
    "        print('updating', file)\n",
    "    return changed #오디오파일이 제대로 된건지, 이상한 부분은 없는지 check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f7c5bf3-1193-4279-b2fe-5815ac37b37e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shin3875: false 0 / 100\n",
      "\n",
      "InkooJeon: false 0 / 100\n",
      "\n",
      "11jeonghy: false 0 / 100\n",
      "\n",
      "son: false 0 / 100\n",
      "\n",
      "YouYeNa: false 0 / 100\n",
      "\n",
      "Dandyst: false 0 / 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainroot = 'segmented-train'\n",
    "'''\n",
    "labels_all = {'11jeonghy', \n",
    "                'Dandyst', \n",
    "                'InkooJeon',\n",
    "                'YouYeNa',\n",
    "                'chlee',\n",
    "                'deokkyukwon',\n",
    "                'do',\n",
    "                'kyeong',\n",
    "                'ohjihyeon',\n",
    "                'son',\n",
    "               }\n",
    "''' #원래 사람이 이렇게 있었는데 덕규랑 지현이 뺀거\n",
    "labels_train = {'11jeonghy', \n",
    "                'Dandyst', \n",
    "                'InkooJeon',\n",
    "                'shin3875',\n",
    "                'YouYeNa',\n",
    "                'son',\n",
    "               } #만든것\n",
    "\n",
    "# check\n",
    "Fs = 16000 #음성신호 \n",
    "for subname in labels_train:\n",
    "    num_files = 0\n",
    "    num_false_files = 0\n",
    "    for w in range(10):\n",
    "        for trial in range(10):\n",
    "            basename = '%d/kdigits%d-%d.wav'%(w,trial,w)\n",
    "            file = addpath(trainroot, addpath(subname, basename)) #특정 파일을 가져오는 명령어\n",
    "            num_files += 1\n",
    "            if check_audio_file(file, Fs, True) == False:\n",
    "                num_false_files += 1 #파일이 맞는지 check\n",
    "    print('%s: false %d / %d\\n'%(subname, num_false_files, num_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "342c0678-3f89-45cb-af59-b6f088ef6058",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kyeong: false 0 / 100\n",
      "\n",
      "do: false 0 / 100\n",
      "\n",
      "chlee: false 0 / 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valroot = 'segmented-val'\n",
    "valclean = addpath(valroot, 'org')\n",
    "labels_val = {\n",
    "                'chlee',\n",
    "                'do',\n",
    "                'kyeong',\n",
    "               }\n",
    "\n",
    "# check\n",
    "Fs = 16000\n",
    "for subname in labels_val:\n",
    "    num_files = 0\n",
    "    num_false_files = 0\n",
    "    for w in range(10):\n",
    "        for trial in range(10):\n",
    "            basename = '%d/kdigits%d-%d.wav'%(w,trial,w)\n",
    "            file = addpath(valclean, addpath(subname, basename))\n",
    "            num_files += 1\n",
    "            if check_audio_file(file, Fs, True) == False:\n",
    "                num_false_files += 1\n",
    "    print('%s: false %d / %d\\n'%(subname, num_false_files, num_files)) #Validation 파일도 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4073a311-4b6c-44c0-aa1c-4d9d666ad3ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### HMM training and test 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24b52602-9088-4556-9b0e-259ede899922",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StartProbPrior=\n",
      "[1. 0. 0.]\n",
      "TransMatPrior=\n",
      "[[0.5 0.5 0. ]\n",
      " [0.  0.5 0.5]\n",
      " [0.  0.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from scikits.talkbox.features import mfcc\n",
    "#librosa.feature.mfcc(*, y=None, sr=22050, S=None, n_mfcc=20, dct_type=2, norm='ortho', lifter=0, **kwargs)[source]\n",
    "from librosa.feature import mfcc\n",
    "from scipy.io import wavfile\n",
    "from hmmlearn import hmm\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import scipy.stats as sp\n",
    "from time import time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "############################################################################################## \n",
    "# extract MFCC features\n",
    "def extmfcc(file):\n",
    "    samplerate, d = wavfile.read(file)\n",
    "    #features.append(mfcc(d, nwin=int(samplerate * 0.03), fs=samplerate, nceps= 6)[0])\n",
    "    x = np.float32(d)\n",
    "    hop=samplerate//100\n",
    "    mc = mfcc(y=x, sr=samplerate, n_mfcc=num_mfcc, hop_length=hop, win_length=hop*2)\n",
    "    return np.transpose(mc, (1,0))\n",
    "\n",
    "def initByBakis(inumstates, ibakisLevel):\n",
    "    startprobPrior = np.zeros(inumstates)\n",
    "    startprobPrior[0: ibakisLevel - 1] = 1/float((ibakisLevel - 1))\n",
    "    transmatPrior = getTransmatPrior(inumstates, ibakisLevel)\n",
    "    return startprobPrior, transmatPrior\n",
    "\n",
    "def getTransmatPrior(inumstates, ibakisLevel):\n",
    "    transmatPrior = (1 / float(ibakisLevel)) * np.eye(inumstates)\n",
    "\n",
    "    for i in range(inumstates - (ibakisLevel - 1)):\n",
    "        for j in range(ibakisLevel - 1):\n",
    "            transmatPrior[i, i + j + 1] = 1. / ibakisLevel\n",
    "\n",
    "    for i in range(inumstates - ibakisLevel + 1, inumstates):\n",
    "        for j in range(inumstates - i - j):\n",
    "            transmatPrior[i, i + j] = 1. / (inumstates - i)\n",
    "\n",
    "    return transmatPrior\n",
    "\n",
    "\n",
    "############################################################################################## \n",
    "# hyperparameters - CHANGE THEM TO IMPROVE PERFORMANCE\n",
    "# 1. number of MFCC (feature dimension)\n",
    "num_mfcc = 6\n",
    "#num_mfcc = 10\n",
    "#num_mfcc = 13\n",
    "# 2. Parameters needed to train GMMHMM\n",
    "m_num_of_HMMStates = 3  # number of states\n",
    "m_num_of_mixtures = 2  # number of mixtures for each hidden state\n",
    "m_covarianceType = 'diag'  # covariance type\n",
    "m_n_iter = 10  # number of iterations\n",
    "m_bakisLevel = 2\n",
    "m_startprobPrior, m_transmatPrior = initByBakis(m_num_of_HMMStates,m_bakisLevel)\n",
    "print(\"StartProbPrior=\"); print(m_startprobPrior)\n",
    "print(\"TransMatPrior=\"); print(m_transmatPrior)\n",
    "\n",
    "\n",
    "############################################################################################## \n",
    "# acoustic model definition\n",
    "class SpeechModel:\n",
    "    def __init__(self,Class,label):\n",
    "        self.traindata = np.zeros((0,num_mfcc))\n",
    "        self.Class = Class\n",
    "        self.label = label\n",
    "        self.model  = hmm.GMMHMM(n_components = m_num_of_HMMStates, n_mix = m_num_of_mixtures, \\\n",
    "                transmat_prior = m_transmatPrior, startprob_prior = m_startprobPrior, \\\n",
    "                covariance_type = m_covarianceType, n_iter = m_n_iter)\n",
    "\n",
    "##################################################################################\n",
    "# folder structure:\n",
    "#  ${rootpath} / ${speaker_name} / m:0-9 / ${tag}[t:0-${numtrials}]-[m:0-9]\n",
    "#    m:0-9 model number\n",
    "#    t:0-{numtrials} trial number\n",
    "#  example: train_digits('segmented-train', {'gjang', 'do', 'son'}, 'kdigis', 10) \n",
    "#           will train with\n",
    "#    segmented-train/gjang/0/kdigits0-0.wav\n",
    "#    segmented-train/gjang/0/kdigits1-0.wav\n",
    "#    ...\n",
    "#    segmented-train/son/9/kdigits8-9.wav\n",
    "#    segmented-train/son/9/kdigits9-9.wav\n",
    "##################################################################################\n",
    "def train_digits(rootpath, speakers, tag, num_trials=10):    \n",
    "    ############################################################################################## \n",
    "    # 1. find files\n",
    "    #    for user \"gjang\", digit 2, recording trial 0 (1st)\n",
    "    #    \"segmented/gjang/2/kdigits0-2.wav\"\n",
    "    # 2. extract MFCC features for training and testing\n",
    "    #    for each digit, indexes 4 and 9 for test, and the rest for training\n",
    "\n",
    "    #fpaths = []\n",
    "    #labels = []\n",
    "    spoken = []\n",
    "    m_trainingsetfeatures = []\n",
    "    m_trainingsetlabels = []\n",
    "\n",
    "    count = 0\n",
    "    for username in speakers:\n",
    "        apath2 = addpath(rootpath, username)    # example: segmented/gjang\n",
    "        for ii in range(10):   #dnum in os.listdir(apath2):\n",
    "            dnum = str(ii)\n",
    "            apath3 = addpath(apath2, dnum)     # example: segmented/gjang/2\n",
    "            if dnum not in spoken:\n",
    "                spoken.append(dnum)\n",
    "            for trial in range(num_trials):\n",
    "                file = addpath(apath3,\"{}{}-{}.wav\".format(tag,trial,dnum))      # segmented/gjang/2/kdigits0-2.wav\n",
    "                mc = extmfcc(file)\n",
    "\n",
    "                # display file names for the first 20 files only\n",
    "                count += 1\n",
    "                if count <= 20:\n",
    "                    print(file, dnum, end=' '); print(mc.shape, end=' ')\n",
    "                elif count == 21:\n",
    "                    print('...'); print('')\n",
    "\n",
    "                m_trainingsetfeatures.append(mc)\n",
    "                m_trainingsetlabels.append(dnum)\n",
    "\n",
    "    print('Words spoken:', spoken)\n",
    "    #print(\"number of labels and features = %d, %d\" % ( len(labels), len(features) ))\n",
    "    #print(\"feature shape = \", end='')\n",
    "    #print(features[0].shape)\n",
    "\n",
    "    ############################################################################################## \n",
    "    ntrain = len(m_trainingsetlabels)\n",
    "\n",
    "    print(\"[training] number of labels and features = %d, %d\" % \n",
    "            ( len(m_trainingsetlabels), len(m_trainingsetfeatures)) )\n",
    "    print ('Loading data completed')\n",
    "\n",
    "    ############################################################################################## \n",
    "    # model initialization\n",
    "    gmmhmmindexdict = {}\n",
    "    index = 0\n",
    "    for word in spoken:\n",
    "        gmmhmmindexdict[word] = index\n",
    "        index = index +1\n",
    "\n",
    "    ############################################################################################## \n",
    "    # training GMMHMM Models \n",
    "    start = time()\n",
    "\n",
    "    speechmodels = [None] * len(spoken)\n",
    "    for key in gmmhmmindexdict:\n",
    "        speechmodels[gmmhmmindexdict[key]] = SpeechModel(gmmhmmindexdict[key],key)\n",
    "\n",
    "    for i in range(0,len(m_trainingsetfeatures)):\n",
    "         for j in range(0,len(speechmodels)):\n",
    "             if int(speechmodels[j].Class) == int(gmmhmmindexdict[m_trainingsetlabels[i]]):\n",
    "                speechmodels[j].traindata = np.concatenate((speechmodels[j].traindata , m_trainingsetfeatures[i]))\n",
    "\n",
    "    for speechmodel in speechmodels:\n",
    "        speechmodel.model.fit(speechmodel.traindata)\n",
    "\n",
    "    print ('Training completed -- {0} GMM-HMM models are built for {0} different types of words'.format(len(spoken)))\n",
    "    print('time elapsed: %.2f seconds' % ( time() - start ))\n",
    "    print (\" \"); print(\" \")\n",
    "    \n",
    "    return speechmodels, gmmhmmindexdict\n",
    "\n",
    "    '''\n",
    "    ############################################################################################## \n",
    "    # testing\n",
    "    print(\"Prediction with training data started\")\n",
    "    m_PredictionlabelList = []\n",
    "\n",
    "    for i in range(0,len(m_testingsetfeatures)):\n",
    "        scores = []\n",
    "        for speechmodel in speechmodels:\n",
    "             scores.append(speechmodel.model.score(m_testingsetfeatures[i]))\n",
    "        id  = scores.index(max(scores))\n",
    "        m_PredictionlabelList.append(speechmodels[id].Class)\n",
    "        print(str(np.round(scores, 3)) + \" \" + str(max(np.round(scores, 3))) +\" \"+\":\"+ speechmodels[id].label)\n",
    "\n",
    "    accuracy = 0.0\n",
    "    count = 0\n",
    "    print(\"\")\n",
    "    print(\"Prediction for Testing DataSet:\")\n",
    "\n",
    "    for i in range(0,len(m_testingsetlabels)):\n",
    "        print( \"Label\"+str(i+1)+\":\"+m_testingsetlabels[i])\n",
    "        if gmmhmmindexdict[m_testingsetlabels[i]] == m_PredictionlabelList[i]:\n",
    "           count = count+1\n",
    "\n",
    "    accuracy = 100.0*count/float(len(m_testingsetlabels))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"accuracy =\"+str(accuracy))\n",
    "    print(\"\")\n",
    "\n",
    "    ############################################################################################## \n",
    "    # end of testing\n",
    "    ############################################################################################## \n",
    "    '''#교수님 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08525bf8-b5bc-4b28-87fa-f304681efa94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from scikits.talkbox.features import mfcc\n",
    "#librosa.feature.mfcc(*, y=None, sr=22050, S=None, n_mfcc=20, dct_type=2, norm='ortho', lifter=0, **kwargs)[source]\n",
    "from librosa.feature import mfcc\n",
    "from scipy.io import wavfile\n",
    "from hmmlearn import hmm\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import scipy.stats as sp\n",
    "from time import time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "##################################################################################\n",
    "# folder structure:\n",
    "#  ${rootpath} / ${speaker_name} / m:0-9 / ${tag}[t:0-${numtrials}]-[m:0-9]\n",
    "#    m:0-9 model number\n",
    "#    t:0-{numtrials} trial number\n",
    "#  example: train_digits('segmented-train', {'gjang', 'do', 'son'}, 'kdigis', 10) \n",
    "#           will train with\n",
    "#    segmented-train/gjang/0/kdigits0-0.wav\n",
    "#    segmented-train/gjang/0/kdigits1-0.wav\n",
    "#    ...\n",
    "#    segmented-train/son/9/kdigits8-9.wav\n",
    "#    segmented-train/son/9/kdigits9-9.wav\n",
    "##################################################################################\n",
    "def validation_digits(speechmodels, gmmhmmindexdict, rootpath, speakers, tag, num_trials=10):    \n",
    "\n",
    "    ############################################################################################## \n",
    "    # 1. find files\n",
    "    #    for user \"gjang\", digit 2, recording trial 0 (1st)\n",
    "    #    \"segmented/gjang/2/kdigits0-2.wav\"\n",
    "    # 2. extract MFCC features for training and testing\n",
    "    #    for each digit, indexes 4 and 9 for test, and the rest for training\n",
    "\n",
    "    #fpaths = []\n",
    "    #labels = []\n",
    "    spoken = []\n",
    "    m_features = []\n",
    "    m_labels = []\n",
    "\n",
    "    count = 0\n",
    "    for username in speakers:\n",
    "        apath2 = addpath(rootpath, username)    # example: segmented/gjang\n",
    "        for ii in range(10):   #dnum in os.listdir(apath2):\n",
    "            dnum = str(ii)\n",
    "            apath3 = addpath(apath2, dnum)     # example: segmented/gjang/2\n",
    "            if dnum not in spoken:\n",
    "                spoken.append(dnum)\n",
    "            for trial in range(num_trials):\n",
    "                file = addpath(apath3,\"{}{}-{}.wav\".format(tag,trial,dnum))      # segmented/gjang/2/kdigits0-2.wav\n",
    "                mc = extmfcc(file)\n",
    "\n",
    "                # display file names for the first 20 files only\n",
    "                count += 1\n",
    "                if count <= 20:\n",
    "                    print(file, dnum, end=' '); print(mc.shape, end=' ')\n",
    "                elif count == 21:\n",
    "                    print('...'); print('')\n",
    "\n",
    "                m_features.append(mc)\n",
    "                m_labels.append(dnum)\n",
    "\n",
    "    print('Words spoken:', spoken)\n",
    "    #print(\"number of labels and features = %d, %d\" % ( len(labels), len(features) ))\n",
    "    #print(\"feature shape = \", end='')\n",
    "    #print(features[0].shape)\n",
    "\n",
    "    ############################################################################################## \n",
    "    print(\"[validation] number of labels and features = %d, %d\" % ( len(m_labels), len(m_features)) )\n",
    "    print ('Loading data completed')\n",
    "\n",
    "    ############################################################################################## \n",
    "    # testing\n",
    "    print(\"Prediction started\")\n",
    "    m_PredictionlabelList = []\n",
    "\n",
    "    for i in range(0,len(m_features)):\n",
    "        scores = []\n",
    "        for speechmodel in speechmodels:\n",
    "             scores.append(speechmodel.model.score(m_features[i]))\n",
    "        id  = scores.index(max(scores))\n",
    "        m_PredictionlabelList.append(speechmodels[id].Class)\n",
    "        #print(str(np.round(scores, 3)) + \" \" + str(max(np.round(scores, 3))) +\" \"+\":\"+ speechmodels[id].label)\n",
    "\n",
    "    accuracy = 0.0\n",
    "    count = 0\n",
    "    print(\"\")\n",
    "    print(\"Prediction for Testing DataSet:\")\n",
    "\n",
    "    for i in range(0,len(m_labels)):\n",
    "        #print( \"Label\"+str(i+1)+\":\"+m_labels[i])\n",
    "        if gmmhmmindexdict[m_labels[i]] == m_PredictionlabelList[i]:\n",
    "           count = count+1\n",
    "\n",
    "    accuracy = 100.0*count/float(len(m_labels))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"accuracy =\"+str(accuracy))\n",
    "    print(\"\")\n",
    "\n",
    "    ############################################################################################## \n",
    "    # end of testing 교수님 파일\n",
    "    ############################################################################################## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0d39427-c914-462d-adf7-80a5b3882ca3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segmented-train/shin3875/0/kdigits0-0.wav 0 (105, 6) segmented-train/shin3875/0/kdigits1-0.wav 0 (78, 6) segmented-train/shin3875/0/kdigits2-0.wav 0 (152, 6) segmented-train/shin3875/0/kdigits3-0.wav 0 (78, 6) segmented-train/shin3875/0/kdigits4-0.wav 0 (55, 6) segmented-train/shin3875/0/kdigits5-0.wav 0 (101, 6) segmented-train/shin3875/0/kdigits6-0.wav 0 (92, 6) segmented-train/shin3875/0/kdigits7-0.wav 0 (88, 6) segmented-train/shin3875/0/kdigits8-0.wav 0 (125, 6) segmented-train/shin3875/0/kdigits9-0.wav 0 (69, 6) segmented-train/shin3875/1/kdigits0-1.wav 1 (121, 6) segmented-train/shin3875/1/kdigits1-1.wav 1 (89, 6) segmented-train/shin3875/1/kdigits2-1.wav 1 (149, 6) segmented-train/shin3875/1/kdigits3-1.wav 1 (108, 6) segmented-train/shin3875/1/kdigits4-1.wav 1 (113, 6) segmented-train/shin3875/1/kdigits5-1.wav 1 (91, 6) segmented-train/shin3875/1/kdigits6-1.wav 1 (100, 6) segmented-train/shin3875/1/kdigits7-1.wav 1 (111, 6) segmented-train/shin3875/1/kdigits8-1.wav 1 (114, 6) segmented-train/shin3875/1/kdigits9-1.wav 1 (104, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[training] number of labels and features = 600, 600\n",
      "Loading data completed\n",
      "Training completed -- 10 GMM-HMM models are built for 10 different types of words\n",
      "time elapsed: 3.43 seconds\n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "speechmodels, gmmhmmindexdict = train_digits(trainroot, labels_train, 'kdigits', num_trials=10) #Train 시키는것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afc22385-4942-4995-aef2-8e3a236d73ba",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segmented-train/shin3875/0/kdigits0-0.wav 0 (105, 6) segmented-train/shin3875/0/kdigits1-0.wav 0 (78, 6) segmented-train/shin3875/0/kdigits2-0.wav 0 (152, 6) segmented-train/shin3875/0/kdigits3-0.wav 0 (78, 6) segmented-train/shin3875/0/kdigits4-0.wav 0 (55, 6) segmented-train/shin3875/0/kdigits5-0.wav 0 (101, 6) segmented-train/shin3875/0/kdigits6-0.wav 0 (92, 6) segmented-train/shin3875/0/kdigits7-0.wav 0 (88, 6) segmented-train/shin3875/0/kdigits8-0.wav 0 (125, 6) segmented-train/shin3875/0/kdigits9-0.wav 0 (69, 6) segmented-train/shin3875/1/kdigits0-1.wav 1 (121, 6) segmented-train/shin3875/1/kdigits1-1.wav 1 (89, 6) segmented-train/shin3875/1/kdigits2-1.wav 1 (149, 6) segmented-train/shin3875/1/kdigits3-1.wav 1 (108, 6) segmented-train/shin3875/1/kdigits4-1.wav 1 (113, 6) segmented-train/shin3875/1/kdigits5-1.wav 1 (91, 6) segmented-train/shin3875/1/kdigits6-1.wav 1 (100, 6) segmented-train/shin3875/1/kdigits7-1.wav 1 (111, 6) segmented-train/shin3875/1/kdigits8-1.wav 1 (114, 6) segmented-train/shin3875/1/kdigits9-1.wav 1 (104, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 600, 600\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =39.666666666666664\n",
      "\n",
      "segmented-val/org/kyeong/0/kdigits0-0.wav 0 (110, 6) segmented-val/org/kyeong/0/kdigits1-0.wav 0 (94, 6) segmented-val/org/kyeong/0/kdigits2-0.wav 0 (65, 6) segmented-val/org/kyeong/0/kdigits3-0.wav 0 (108, 6) segmented-val/org/kyeong/0/kdigits4-0.wav 0 (103, 6) segmented-val/org/kyeong/0/kdigits5-0.wav 0 (139, 6) segmented-val/org/kyeong/0/kdigits6-0.wav 0 (98, 6) segmented-val/org/kyeong/0/kdigits7-0.wav 0 (65, 6) segmented-val/org/kyeong/0/kdigits8-0.wav 0 (92, 6) segmented-val/org/kyeong/0/kdigits9-0.wav 0 (90, 6) segmented-val/org/kyeong/1/kdigits0-1.wav 1 (103, 6) segmented-val/org/kyeong/1/kdigits1-1.wav 1 (111, 6) segmented-val/org/kyeong/1/kdigits2-1.wav 1 (86, 6) segmented-val/org/kyeong/1/kdigits3-1.wav 1 (95, 6) segmented-val/org/kyeong/1/kdigits4-1.wav 1 (109, 6) segmented-val/org/kyeong/1/kdigits5-1.wav 1 (85, 6) segmented-val/org/kyeong/1/kdigits6-1.wav 1 (96, 6) segmented-val/org/kyeong/1/kdigits7-1.wav 1 (105, 6) segmented-val/org/kyeong/1/kdigits8-1.wav 1 (115, 6) segmented-val/org/kyeong/1/kdigits9-1.wav 1 (94, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 300, 300\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =28.333333333333332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validation_digits(speechmodels, gmmhmmindexdict, trainroot, labels_train, 'kdigits', num_trials=10)\n",
    "validation_digits(speechmodels, gmmhmmindexdict, valclean, labels_val, 'kdigits', num_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cdf79c-6d34-4c91-bcf3-993188dcb566",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### noise 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e77f9be2-c131-40fc-99e9-d42b7c645212",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../audio/car.wav (175745,) [-0.01342773 -0.0222168  -0.02905273 ... -0.0390625  -0.03930664\n",
      " -0.04086304]\n",
      "../audio/car_wideband.wav (175745,) [-0.05984497 -0.14807129 -0.14700317 ... -0.10241699 -0.10253906\n",
      " -0.09594727]\n",
      "Fs = 16000, Ns = 160, Nf = 320, NFFT = 512, hNo = 257\n"
     ]
    }
   ],
   "source": [
    "audioinputpath = '../audio'\n",
    "noisefile  = addpath(audioinputpath, 'car.wav')\n",
    "wnoisefile  = addpath(audioinputpath, 'car_wideband.wav')   # 넓은 주파수 대역에 분포한 잡음\n",
    "\n",
    "Fs=16000\n",
    "noise, _ = librosa.load(noisefile, sr=Fs, mono=True)\n",
    "wnoise, _ = librosa.load(wnoisefile, sr=Fs, mono=True)\n",
    "# sr: target sampling rate. ‘None’ uses the native sampling rate\n",
    "# mono = True: convert signal to mono\n",
    "\n",
    "print(noisefile, noise.shape, noise)\n",
    "print(wnoisefile, wnoise.shape, wnoise)\n",
    "\n",
    "Ns = int(Fs*Ts)    # shift number of samples\n",
    "Nf = int(Fs*Tf)    # frame number of samples\n",
    "NFFT = int(2**(np.ceil(np.log2(Nf))))   # Nf보다 크거나 같은 2의 거듭제곱을 NFFT 로 정의\n",
    "hNo = NFFT//2+1\n",
    "print('Fs = %d, Ns = %d, Nf = %d, NFFT = %d, hNo = %d' % (Fs, Ns, Nf, NFFT, hNo)) #audio/car 파일 삽입"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf48972-47cc-4af7-8de7-96a8dd40676f",
   "metadata": {},
   "source": [
    "__generate noisy speech with various SNRs__\n",
    "- 음성과 잡음의 상대적 크기에 따라 잡음의 효과를 time domain, spectrogram, 그리고 들어서 확인해 본다.\n",
    "- mixed input $x[t]$ 를 다음과 같이 생성한다.\n",
    "$$ x[t] = s[t] + 10^{-r/20} \\frac{\\sigma_{s}}{\\sigma_{n}} n[t] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "620a2571-2da9-4505-b211-2cfa13816693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_mixed_signals_2(speech, noise, SNRs, isdraw=False):\n",
    "    std_s = np.sqrt(np.mean(speech**2))\n",
    "    std_n = np.sqrt(np.mean(noise[:len(speech)]**2))\n",
    "    mixedSig = []\n",
    "    for snr in SNRs:\n",
    "        gain = np.power(10, -snr/20)\n",
    "        gn = noise[:len(speech)]/std_n*std_s*gain\n",
    "        m = speech + gn\n",
    "        mixedSig.append(m)\n",
    "\n",
    "    return mixedSig #lab4/5에 만든 코드 음성신호에 노이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "754fc25f-425b-4277-885b-411d5d65f067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "audioroot = valroot\n",
    "audioclean = valclean\n",
    "labels = labels_val\n",
    "noisyroots = [addpath(audioroot,'nbnSNR'), addpath(audioroot,'wbnSNR')] \n",
    "SNRs = [10, 0, -10] #노이즈 종류는 총 3개 -> 결국 총 6개\n",
    "\n",
    "for subname in labels:\n",
    "    num_files = 0\n",
    "    for w in range(10):\n",
    "        for trial in range(10):\n",
    "            basename = '%d/kdigits%d-%d.wav'%(w,trial,w)\n",
    "            infile = addpath(audioclean, addpath(subname, basename))            \n",
    "            num_files += 1\n",
    "            \n",
    "            signal, Fs = librosa.load(infile, sr=Fs, mono=True)\n",
    "            nbnsig = generate_mixed_signals_2(signal, noise, SNRs, False)\n",
    "            wbnsig = generate_mixed_signals_2(signal, wnoise, SNRs, False)\n",
    "            noisy = [nbnsig, wbnsig] #노이즈 넣는 부분\n",
    "            \n",
    "            for jj in range(len(noisy)):\n",
    "                for n in range(len(noisy[jj])):\n",
    "                    outfile = addpath('%s%d'%(noisyroots[jj],SNRs[n]), addpath(subname, basename))\n",
    "                    wav.writewav(outfile, Fs, noisy[jj][n], maxval=1.0) #저장\n",
    "\n",
    "outputpaths = []\n",
    "for jj in range(len(noisy)):\n",
    "    for n in range(len(noisy[jj])):\n",
    "        outputpaths.append('%s%d'%(noisyroots[jj],SNRs[n])) #노이즈 추가된 파일 경로 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54809aa7-63c6-4e11-b42e-8f59ef3495df",
   "metadata": {},
   "source": [
    "Noise model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46abea0e-484f-4dd2-ba19-d66a44178ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "testing segmented-val/nbnSNR10\n",
      "segmented-val/nbnSNR10/kyeong/0/kdigits0-0.wav 0 (110, 6) segmented-val/nbnSNR10/kyeong/0/kdigits1-0.wav 0 (94, 6) segmented-val/nbnSNR10/kyeong/0/kdigits2-0.wav 0 (65, 6) segmented-val/nbnSNR10/kyeong/0/kdigits3-0.wav 0 (108, 6) segmented-val/nbnSNR10/kyeong/0/kdigits4-0.wav 0 (103, 6) segmented-val/nbnSNR10/kyeong/0/kdigits5-0.wav 0 (139, 6) segmented-val/nbnSNR10/kyeong/0/kdigits6-0.wav 0 (98, 6) segmented-val/nbnSNR10/kyeong/0/kdigits7-0.wav 0 (65, 6) segmented-val/nbnSNR10/kyeong/0/kdigits8-0.wav 0 (92, 6) segmented-val/nbnSNR10/kyeong/0/kdigits9-0.wav 0 (90, 6) segmented-val/nbnSNR10/kyeong/1/kdigits0-1.wav 1 (103, 6) segmented-val/nbnSNR10/kyeong/1/kdigits1-1.wav 1 (111, 6) segmented-val/nbnSNR10/kyeong/1/kdigits2-1.wav 1 (86, 6) segmented-val/nbnSNR10/kyeong/1/kdigits3-1.wav 1 (95, 6) segmented-val/nbnSNR10/kyeong/1/kdigits4-1.wav 1 (109, 6) segmented-val/nbnSNR10/kyeong/1/kdigits5-1.wav 1 (85, 6) segmented-val/nbnSNR10/kyeong/1/kdigits6-1.wav 1 (96, 6) segmented-val/nbnSNR10/kyeong/1/kdigits7-1.wav 1 (105, 6) segmented-val/nbnSNR10/kyeong/1/kdigits8-1.wav 1 (115, 6) segmented-val/nbnSNR10/kyeong/1/kdigits9-1.wav 1 (94, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 300, 300\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =26.0\n",
      "\n",
      "--------------------------------\n",
      "testing segmented-val/nbnSNR0\n",
      "segmented-val/nbnSNR0/kyeong/0/kdigits0-0.wav 0 (110, 6) segmented-val/nbnSNR0/kyeong/0/kdigits1-0.wav 0 (94, 6) segmented-val/nbnSNR0/kyeong/0/kdigits2-0.wav 0 (65, 6) segmented-val/nbnSNR0/kyeong/0/kdigits3-0.wav 0 (108, 6) segmented-val/nbnSNR0/kyeong/0/kdigits4-0.wav 0 (103, 6) segmented-val/nbnSNR0/kyeong/0/kdigits5-0.wav 0 (139, 6) segmented-val/nbnSNR0/kyeong/0/kdigits6-0.wav 0 (98, 6) segmented-val/nbnSNR0/kyeong/0/kdigits7-0.wav 0 (65, 6) segmented-val/nbnSNR0/kyeong/0/kdigits8-0.wav 0 (92, 6) segmented-val/nbnSNR0/kyeong/0/kdigits9-0.wav 0 (90, 6) segmented-val/nbnSNR0/kyeong/1/kdigits0-1.wav 1 (103, 6) segmented-val/nbnSNR0/kyeong/1/kdigits1-1.wav 1 (111, 6) segmented-val/nbnSNR0/kyeong/1/kdigits2-1.wav 1 (86, 6) segmented-val/nbnSNR0/kyeong/1/kdigits3-1.wav 1 (95, 6) segmented-val/nbnSNR0/kyeong/1/kdigits4-1.wav 1 (109, 6) segmented-val/nbnSNR0/kyeong/1/kdigits5-1.wav 1 (85, 6) segmented-val/nbnSNR0/kyeong/1/kdigits6-1.wav 1 (96, 6) segmented-val/nbnSNR0/kyeong/1/kdigits7-1.wav 1 (105, 6) segmented-val/nbnSNR0/kyeong/1/kdigits8-1.wav 1 (115, 6) segmented-val/nbnSNR0/kyeong/1/kdigits9-1.wav 1 (94, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 300, 300\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =17.0\n",
      "\n",
      "--------------------------------\n",
      "testing segmented-val/nbnSNR-10\n",
      "segmented-val/nbnSNR-10/kyeong/0/kdigits0-0.wav 0 (110, 6) segmented-val/nbnSNR-10/kyeong/0/kdigits1-0.wav 0 (94, 6) segmented-val/nbnSNR-10/kyeong/0/kdigits2-0.wav 0 (65, 6) segmented-val/nbnSNR-10/kyeong/0/kdigits3-0.wav 0 (108, 6) segmented-val/nbnSNR-10/kyeong/0/kdigits4-0.wav 0 (103, 6) segmented-val/nbnSNR-10/kyeong/0/kdigits5-0.wav 0 (139, 6) segmented-val/nbnSNR-10/kyeong/0/kdigits6-0.wav 0 (98, 6) segmented-val/nbnSNR-10/kyeong/0/kdigits7-0.wav 0 (65, 6) segmented-val/nbnSNR-10/kyeong/0/kdigits8-0.wav 0 (92, 6) segmented-val/nbnSNR-10/kyeong/0/kdigits9-0.wav 0 (90, 6) segmented-val/nbnSNR-10/kyeong/1/kdigits0-1.wav 1 (103, 6) segmented-val/nbnSNR-10/kyeong/1/kdigits1-1.wav 1 (111, 6) segmented-val/nbnSNR-10/kyeong/1/kdigits2-1.wav 1 (86, 6) segmented-val/nbnSNR-10/kyeong/1/kdigits3-1.wav 1 (95, 6) segmented-val/nbnSNR-10/kyeong/1/kdigits4-1.wav 1 (109, 6) segmented-val/nbnSNR-10/kyeong/1/kdigits5-1.wav 1 (85, 6) segmented-val/nbnSNR-10/kyeong/1/kdigits6-1.wav 1 (96, 6) segmented-val/nbnSNR-10/kyeong/1/kdigits7-1.wav 1 (105, 6) segmented-val/nbnSNR-10/kyeong/1/kdigits8-1.wav 1 (115, 6) segmented-val/nbnSNR-10/kyeong/1/kdigits9-1.wav 1 (94, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 300, 300\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =18.0\n",
      "\n",
      "--------------------------------\n",
      "testing segmented-val/wbnSNR10\n",
      "segmented-val/wbnSNR10/kyeong/0/kdigits0-0.wav 0 (110, 6) segmented-val/wbnSNR10/kyeong/0/kdigits1-0.wav 0 (94, 6) segmented-val/wbnSNR10/kyeong/0/kdigits2-0.wav 0 (65, 6) segmented-val/wbnSNR10/kyeong/0/kdigits3-0.wav 0 (108, 6) segmented-val/wbnSNR10/kyeong/0/kdigits4-0.wav 0 (103, 6) segmented-val/wbnSNR10/kyeong/0/kdigits5-0.wav 0 (139, 6) segmented-val/wbnSNR10/kyeong/0/kdigits6-0.wav 0 (98, 6) segmented-val/wbnSNR10/kyeong/0/kdigits7-0.wav 0 (65, 6) segmented-val/wbnSNR10/kyeong/0/kdigits8-0.wav 0 (92, 6) segmented-val/wbnSNR10/kyeong/0/kdigits9-0.wav 0 (90, 6) segmented-val/wbnSNR10/kyeong/1/kdigits0-1.wav 1 (103, 6) segmented-val/wbnSNR10/kyeong/1/kdigits1-1.wav 1 (111, 6) segmented-val/wbnSNR10/kyeong/1/kdigits2-1.wav 1 (86, 6) segmented-val/wbnSNR10/kyeong/1/kdigits3-1.wav 1 (95, 6) segmented-val/wbnSNR10/kyeong/1/kdigits4-1.wav 1 (109, 6) segmented-val/wbnSNR10/kyeong/1/kdigits5-1.wav 1 (85, 6) segmented-val/wbnSNR10/kyeong/1/kdigits6-1.wav 1 (96, 6) segmented-val/wbnSNR10/kyeong/1/kdigits7-1.wav 1 (105, 6) segmented-val/wbnSNR10/kyeong/1/kdigits8-1.wav 1 (115, 6) segmented-val/wbnSNR10/kyeong/1/kdigits9-1.wav 1 (94, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 300, 300\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =21.666666666666668\n",
      "\n",
      "--------------------------------\n",
      "testing segmented-val/wbnSNR0\n",
      "segmented-val/wbnSNR0/kyeong/0/kdigits0-0.wav 0 (110, 6) segmented-val/wbnSNR0/kyeong/0/kdigits1-0.wav 0 (94, 6) segmented-val/wbnSNR0/kyeong/0/kdigits2-0.wav 0 (65, 6) segmented-val/wbnSNR0/kyeong/0/kdigits3-0.wav 0 (108, 6) segmented-val/wbnSNR0/kyeong/0/kdigits4-0.wav 0 (103, 6) segmented-val/wbnSNR0/kyeong/0/kdigits5-0.wav 0 (139, 6) segmented-val/wbnSNR0/kyeong/0/kdigits6-0.wav 0 (98, 6) segmented-val/wbnSNR0/kyeong/0/kdigits7-0.wav 0 (65, 6) segmented-val/wbnSNR0/kyeong/0/kdigits8-0.wav 0 (92, 6) segmented-val/wbnSNR0/kyeong/0/kdigits9-0.wav 0 (90, 6) segmented-val/wbnSNR0/kyeong/1/kdigits0-1.wav 1 (103, 6) segmented-val/wbnSNR0/kyeong/1/kdigits1-1.wav 1 (111, 6) segmented-val/wbnSNR0/kyeong/1/kdigits2-1.wav 1 (86, 6) segmented-val/wbnSNR0/kyeong/1/kdigits3-1.wav 1 (95, 6) segmented-val/wbnSNR0/kyeong/1/kdigits4-1.wav 1 (109, 6) segmented-val/wbnSNR0/kyeong/1/kdigits5-1.wav 1 (85, 6) segmented-val/wbnSNR0/kyeong/1/kdigits6-1.wav 1 (96, 6) segmented-val/wbnSNR0/kyeong/1/kdigits7-1.wav 1 (105, 6) segmented-val/wbnSNR0/kyeong/1/kdigits8-1.wav 1 (115, 6) segmented-val/wbnSNR0/kyeong/1/kdigits9-1.wav 1 (94, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 300, 300\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =17.333333333333332\n",
      "\n",
      "--------------------------------\n",
      "testing segmented-val/wbnSNR-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segmented-val/wbnSNR-10/kyeong/0/kdigits0-0.wav 0 (110, 6) segmented-val/wbnSNR-10/kyeong/0/kdigits1-0.wav 0 (94, 6) segmented-val/wbnSNR-10/kyeong/0/kdigits2-0.wav 0 (65, 6) segmented-val/wbnSNR-10/kyeong/0/kdigits3-0.wav 0 (108, 6) segmented-val/wbnSNR-10/kyeong/0/kdigits4-0.wav 0 (103, 6) segmented-val/wbnSNR-10/kyeong/0/kdigits5-0.wav 0 (139, 6) segmented-val/wbnSNR-10/kyeong/0/kdigits6-0.wav 0 (98, 6) segmented-val/wbnSNR-10/kyeong/0/kdigits7-0.wav 0 (65, 6) segmented-val/wbnSNR-10/kyeong/0/kdigits8-0.wav 0 (92, 6) segmented-val/wbnSNR-10/kyeong/0/kdigits9-0.wav 0 (90, 6) segmented-val/wbnSNR-10/kyeong/1/kdigits0-1.wav 1 (103, 6) segmented-val/wbnSNR-10/kyeong/1/kdigits1-1.wav 1 (111, 6) segmented-val/wbnSNR-10/kyeong/1/kdigits2-1.wav 1 (86, 6) segmented-val/wbnSNR-10/kyeong/1/kdigits3-1.wav 1 (95, 6) segmented-val/wbnSNR-10/kyeong/1/kdigits4-1.wav 1 (109, 6) segmented-val/wbnSNR-10/kyeong/1/kdigits5-1.wav 1 (85, 6) segmented-val/wbnSNR-10/kyeong/1/kdigits6-1.wav 1 (96, 6) segmented-val/wbnSNR-10/kyeong/1/kdigits7-1.wav 1 (105, 6) segmented-val/wbnSNR-10/kyeong/1/kdigits8-1.wav 1 (115, 6) segmented-val/wbnSNR-10/kyeong/1/kdigits9-1.wav 1 (94, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 300, 300\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =11.333333333333334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for path in outputpaths:\n",
    "    print('--------------------------------')\n",
    "    print('testing', path)\n",
    "    validation_digits(speechmodels, gmmhmmindexdict, path, labels, 'kdigits', num_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce5330b-3c8a-4170-aedf-1376dceec3cf",
   "metadata": {},
   "source": [
    "## EPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82cbca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "import os\n",
    "\n",
    "def split_audio(fpath, outputdir, prefix):\n",
    "    silence_threshold=-40\n",
    "    min_silence_duration=500 \n",
    "    num_splits=10\n",
    "    audio = AudioSegment.from_file(fpath)\n",
    " \n",
    "    if not os.path.exists(outputdir): # 경로가 없으면 폴더 생성\n",
    "        os.makedirs(outputdir)\n",
    "    \n",
    "    # 신호가 낮은 부분을 감지해서 파일을 잘라냄\n",
    "    file_num = split_on_silence(audio, min_silence_len=min_silence_duration, silence_thresh=silence_threshold, keep_silence=500)\n",
    "    \n",
    "    for i, number in enumerate(file_num):\n",
    "        if i >= num_splits:\n",
    "            break\n",
    "            \n",
    "        output_file = os.path.join(outputdir, f\"kdigits{i}-{prefix}.wav\") #잘라낸 파일을 저장\n",
    "        number.export(output_file, format=\"wav\") #https://github.com/worldpeace21/preprocess/blob/master/utils_for_audio.py 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5446b158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#org/gjang 폴더의 음성 파일을 split\n",
    "\n",
    "for i in range(10) : \n",
    "    a = f\"unsegmented-test/org/gjang/kdigits{i}.wav\"\n",
    "    a1 = f\"unsegmented-test/org/gjang/{i}/\"\n",
    "    a2 = f\"{i}\"\n",
    "    split_audio(fpath = a, outputdir = a1, prefix = a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b907fc04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsegmented-test/org/gjang/0/kdigits0-0.wav 0 (129, 6) unsegmented-test/org/gjang/0/kdigits1-0.wav 0 (125, 6) unsegmented-test/org/gjang/0/kdigits2-0.wav 0 (119, 6) unsegmented-test/org/gjang/0/kdigits3-0.wav 0 (120, 6) unsegmented-test/org/gjang/0/kdigits4-0.wav 0 (132, 6) unsegmented-test/org/gjang/0/kdigits5-0.wav 0 (135, 6) unsegmented-test/org/gjang/0/kdigits6-0.wav 0 (137, 6) unsegmented-test/org/gjang/0/kdigits7-0.wav 0 (137, 6) unsegmented-test/org/gjang/0/kdigits8-0.wav 0 (136, 6) unsegmented-test/org/gjang/0/kdigits9-0.wav 0 (124, 6) unsegmented-test/org/gjang/1/kdigits0-1.wav 1 (125, 6) unsegmented-test/org/gjang/1/kdigits1-1.wav 1 (119, 6) unsegmented-test/org/gjang/1/kdigits2-1.wav 1 (122, 6) unsegmented-test/org/gjang/1/kdigits3-1.wav 1 (121, 6) unsegmented-test/org/gjang/1/kdigits4-1.wav 1 (131, 6) unsegmented-test/org/gjang/1/kdigits5-1.wav 1 (128, 6) unsegmented-test/org/gjang/1/kdigits6-1.wav 1 (131, 6) unsegmented-test/org/gjang/1/kdigits7-1.wav 1 (134, 6) unsegmented-test/org/gjang/1/kdigits8-1.wav 1 (136, 6) unsegmented-test/org/gjang/1/kdigits9-1.wav 1 (123, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 100, 100\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =9.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "audioclean = addpath('unsegmented-test','org')\n",
    "labels = ['gjang']\n",
    "validation_digits(speechmodels, gmmhmmindexdict, audioclean, labels, 'kdigits', num_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bfc336",
   "metadata": {},
   "source": [
    "## nbn, wbn (0, 10, -10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ccc07f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#폴더 생성\n",
    "for i in (\"nbn\", \"wbn\"):\n",
    "    for j in (\"0\", \"10\", \"-10\"):\n",
    "        for k in range(10):\n",
    "            output_dir = f\"unsegmented-test/{i}SNR{j}/gjang/{k}\"\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0221c7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "audioroot = 'unsegmented-test'\n",
    "noisyroots = [addpath(audioroot,'nbnSNR'), addpath(audioroot,'wbnSNR')] \n",
    "SNRs = [10, 0, -10] #노이즈 종류는 총 3개 -> 결국 총 6개\n",
    "\n",
    "for subname in labels:\n",
    "    num_files = 0\n",
    "    for w in range(10):\n",
    "        for trial in range(10):\n",
    "            basename = '%d/kdigits%d-%d.wav'%(w,trial,w)\n",
    "            infile = addpath(audioclean, addpath(subname, basename))            \n",
    "            num_files += 1\n",
    "            \n",
    "            signal, Fs = librosa.load(infile, sr=Fs, mono=True)\n",
    "            nbnsig = generate_mixed_signals_2(signal, noise, SNRs, False)\n",
    "            wbnsig = generate_mixed_signals_2(signal, wnoise, SNRs, False)\n",
    "            noisy = [nbnsig, wbnsig] #노이즈 넣는 부분\n",
    "            \n",
    "            for jj in range(len(noisy)):\n",
    "                for n in range(len(noisy[jj])):\n",
    "                    outfile = addpath('%s%d'%(noisyroots[jj],SNRs[n]), addpath(subname, basename))\n",
    "                    wav.writewav(outfile, Fs, noisy[jj][n], maxval=1.0) #저장\n",
    "\n",
    "outputpaths = []\n",
    "for jj in range(len(noisy)):\n",
    "    for n in range(len(noisy[jj])):\n",
    "        outputpaths.append('%s%d'%(noisyroots[jj],SNRs[n])) #노이즈 추가된 파일 경로 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2b3fba69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsegmented-test/nbnSNR0/gjang/0/kdigits0-0.wav 0 (129, 6) unsegmented-test/nbnSNR0/gjang/0/kdigits1-0.wav 0 (125, 6) unsegmented-test/nbnSNR0/gjang/0/kdigits2-0.wav 0 (119, 6) unsegmented-test/nbnSNR0/gjang/0/kdigits3-0.wav 0 (120, 6) unsegmented-test/nbnSNR0/gjang/0/kdigits4-0.wav 0 (132, 6) unsegmented-test/nbnSNR0/gjang/0/kdigits5-0.wav 0 (135, 6) unsegmented-test/nbnSNR0/gjang/0/kdigits6-0.wav 0 (137, 6) unsegmented-test/nbnSNR0/gjang/0/kdigits7-0.wav 0 (137, 6) unsegmented-test/nbnSNR0/gjang/0/kdigits8-0.wav 0 (136, 6) unsegmented-test/nbnSNR0/gjang/0/kdigits9-0.wav 0 (124, 6) unsegmented-test/nbnSNR0/gjang/1/kdigits0-1.wav 1 (125, 6) unsegmented-test/nbnSNR0/gjang/1/kdigits1-1.wav 1 (119, 6) unsegmented-test/nbnSNR0/gjang/1/kdigits2-1.wav 1 (122, 6) unsegmented-test/nbnSNR0/gjang/1/kdigits3-1.wav 1 (121, 6) unsegmented-test/nbnSNR0/gjang/1/kdigits4-1.wav 1 (131, 6) unsegmented-test/nbnSNR0/gjang/1/kdigits5-1.wav 1 (128, 6) unsegmented-test/nbnSNR0/gjang/1/kdigits6-1.wav 1 (131, 6) unsegmented-test/nbnSNR0/gjang/1/kdigits7-1.wav 1 (134, 6) unsegmented-test/nbnSNR0/gjang/1/kdigits8-1.wav 1 (136, 6) unsegmented-test/nbnSNR0/gjang/1/kdigits9-1.wav 1 (123, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 100, 100\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =10.0\n",
      "\n",
      "unsegmented-test/nbnSNR10/gjang/0/kdigits0-0.wav 0 (129, 6) unsegmented-test/nbnSNR10/gjang/0/kdigits1-0.wav 0 (125, 6) unsegmented-test/nbnSNR10/gjang/0/kdigits2-0.wav 0 (119, 6) unsegmented-test/nbnSNR10/gjang/0/kdigits3-0.wav 0 (120, 6) unsegmented-test/nbnSNR10/gjang/0/kdigits4-0.wav 0 (132, 6) unsegmented-test/nbnSNR10/gjang/0/kdigits5-0.wav 0 (135, 6) unsegmented-test/nbnSNR10/gjang/0/kdigits6-0.wav 0 (137, 6) unsegmented-test/nbnSNR10/gjang/0/kdigits7-0.wav 0 (137, 6) unsegmented-test/nbnSNR10/gjang/0/kdigits8-0.wav 0 (136, 6) unsegmented-test/nbnSNR10/gjang/0/kdigits9-0.wav 0 (124, 6) unsegmented-test/nbnSNR10/gjang/1/kdigits0-1.wav 1 (125, 6) unsegmented-test/nbnSNR10/gjang/1/kdigits1-1.wav 1 (119, 6) unsegmented-test/nbnSNR10/gjang/1/kdigits2-1.wav 1 (122, 6) unsegmented-test/nbnSNR10/gjang/1/kdigits3-1.wav 1 (121, 6) unsegmented-test/nbnSNR10/gjang/1/kdigits4-1.wav 1 (131, 6) unsegmented-test/nbnSNR10/gjang/1/kdigits5-1.wav 1 (128, 6) unsegmented-test/nbnSNR10/gjang/1/kdigits6-1.wav 1 (131, 6) unsegmented-test/nbnSNR10/gjang/1/kdigits7-1.wav 1 (134, 6) unsegmented-test/nbnSNR10/gjang/1/kdigits8-1.wav 1 (136, 6) unsegmented-test/nbnSNR10/gjang/1/kdigits9-1.wav 1 (123, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 100, 100\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =10.0\n",
      "\n",
      "unsegmented-test/nbnSNR-10/gjang/0/kdigits0-0.wav 0 (129, 6) unsegmented-test/nbnSNR-10/gjang/0/kdigits1-0.wav 0 (125, 6) unsegmented-test/nbnSNR-10/gjang/0/kdigits2-0.wav 0 (119, 6) unsegmented-test/nbnSNR-10/gjang/0/kdigits3-0.wav 0 (120, 6) unsegmented-test/nbnSNR-10/gjang/0/kdigits4-0.wav 0 (132, 6) unsegmented-test/nbnSNR-10/gjang/0/kdigits5-0.wav 0 (135, 6) unsegmented-test/nbnSNR-10/gjang/0/kdigits6-0.wav 0 (137, 6) unsegmented-test/nbnSNR-10/gjang/0/kdigits7-0.wav 0 (137, 6) unsegmented-test/nbnSNR-10/gjang/0/kdigits8-0.wav 0 (136, 6) unsegmented-test/nbnSNR-10/gjang/0/kdigits9-0.wav 0 (124, 6) unsegmented-test/nbnSNR-10/gjang/1/kdigits0-1.wav 1 (125, 6) unsegmented-test/nbnSNR-10/gjang/1/kdigits1-1.wav 1 (119, 6) unsegmented-test/nbnSNR-10/gjang/1/kdigits2-1.wav 1 (122, 6) unsegmented-test/nbnSNR-10/gjang/1/kdigits3-1.wav 1 (121, 6) unsegmented-test/nbnSNR-10/gjang/1/kdigits4-1.wav 1 (131, 6) unsegmented-test/nbnSNR-10/gjang/1/kdigits5-1.wav 1 (128, 6) unsegmented-test/nbnSNR-10/gjang/1/kdigits6-1.wav 1 (131, 6) unsegmented-test/nbnSNR-10/gjang/1/kdigits7-1.wav 1 (134, 6) unsegmented-test/nbnSNR-10/gjang/1/kdigits8-1.wav 1 (136, 6) unsegmented-test/nbnSNR-10/gjang/1/kdigits9-1.wav 1 (123, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 100, 100\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =8.0\n",
      "\n",
      "unsegmented-test/wbnSNR0/gjang/0/kdigits0-0.wav 0 (129, 6) unsegmented-test/wbnSNR0/gjang/0/kdigits1-0.wav 0 (125, 6) unsegmented-test/wbnSNR0/gjang/0/kdigits2-0.wav 0 (119, 6) unsegmented-test/wbnSNR0/gjang/0/kdigits3-0.wav 0 (120, 6) unsegmented-test/wbnSNR0/gjang/0/kdigits4-0.wav 0 (132, 6) unsegmented-test/wbnSNR0/gjang/0/kdigits5-0.wav 0 (135, 6) unsegmented-test/wbnSNR0/gjang/0/kdigits6-0.wav 0 (137, 6) unsegmented-test/wbnSNR0/gjang/0/kdigits7-0.wav 0 (137, 6) unsegmented-test/wbnSNR0/gjang/0/kdigits8-0.wav 0 (136, 6) unsegmented-test/wbnSNR0/gjang/0/kdigits9-0.wav 0 (124, 6) unsegmented-test/wbnSNR0/gjang/1/kdigits0-1.wav 1 (125, 6) unsegmented-test/wbnSNR0/gjang/1/kdigits1-1.wav 1 (119, 6) unsegmented-test/wbnSNR0/gjang/1/kdigits2-1.wav 1 (122, 6) unsegmented-test/wbnSNR0/gjang/1/kdigits3-1.wav 1 (121, 6) unsegmented-test/wbnSNR0/gjang/1/kdigits4-1.wav 1 (131, 6) unsegmented-test/wbnSNR0/gjang/1/kdigits5-1.wav 1 (128, 6) unsegmented-test/wbnSNR0/gjang/1/kdigits6-1.wav 1 (131, 6) unsegmented-test/wbnSNR0/gjang/1/kdigits7-1.wav 1 (134, 6) unsegmented-test/wbnSNR0/gjang/1/kdigits8-1.wav 1 (136, 6) unsegmented-test/wbnSNR0/gjang/1/kdigits9-1.wav 1 (123, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 100, 100\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =10.0\n",
      "\n",
      "unsegmented-test/wbnSNR10/gjang/0/kdigits0-0.wav 0 (129, 6) unsegmented-test/wbnSNR10/gjang/0/kdigits1-0.wav 0 (125, 6) unsegmented-test/wbnSNR10/gjang/0/kdigits2-0.wav 0 (119, 6) unsegmented-test/wbnSNR10/gjang/0/kdigits3-0.wav 0 (120, 6) unsegmented-test/wbnSNR10/gjang/0/kdigits4-0.wav 0 (132, 6) unsegmented-test/wbnSNR10/gjang/0/kdigits5-0.wav 0 (135, 6) unsegmented-test/wbnSNR10/gjang/0/kdigits6-0.wav 0 (137, 6) unsegmented-test/wbnSNR10/gjang/0/kdigits7-0.wav 0 (137, 6) unsegmented-test/wbnSNR10/gjang/0/kdigits8-0.wav 0 (136, 6) unsegmented-test/wbnSNR10/gjang/0/kdigits9-0.wav 0 (124, 6) unsegmented-test/wbnSNR10/gjang/1/kdigits0-1.wav 1 (125, 6) unsegmented-test/wbnSNR10/gjang/1/kdigits1-1.wav 1 (119, 6) unsegmented-test/wbnSNR10/gjang/1/kdigits2-1.wav 1 (122, 6) unsegmented-test/wbnSNR10/gjang/1/kdigits3-1.wav 1 (121, 6) unsegmented-test/wbnSNR10/gjang/1/kdigits4-1.wav 1 (131, 6) unsegmented-test/wbnSNR10/gjang/1/kdigits5-1.wav 1 (128, 6) unsegmented-test/wbnSNR10/gjang/1/kdigits6-1.wav 1 (131, 6) unsegmented-test/wbnSNR10/gjang/1/kdigits7-1.wav 1 (134, 6) unsegmented-test/wbnSNR10/gjang/1/kdigits8-1.wav 1 (136, 6) unsegmented-test/wbnSNR10/gjang/1/kdigits9-1.wav 1 (123, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 100, 100\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =11.0\n",
      "\n",
      "unsegmented-test/wbnSNR-10/gjang/0/kdigits0-0.wav 0 (129, 6) unsegmented-test/wbnSNR-10/gjang/0/kdigits1-0.wav 0 (125, 6) unsegmented-test/wbnSNR-10/gjang/0/kdigits2-0.wav 0 (119, 6) unsegmented-test/wbnSNR-10/gjang/0/kdigits3-0.wav 0 (120, 6) unsegmented-test/wbnSNR-10/gjang/0/kdigits4-0.wav 0 (132, 6) unsegmented-test/wbnSNR-10/gjang/0/kdigits5-0.wav 0 (135, 6) unsegmented-test/wbnSNR-10/gjang/0/kdigits6-0.wav 0 (137, 6) unsegmented-test/wbnSNR-10/gjang/0/kdigits7-0.wav 0 (137, 6) unsegmented-test/wbnSNR-10/gjang/0/kdigits8-0.wav 0 (136, 6) unsegmented-test/wbnSNR-10/gjang/0/kdigits9-0.wav 0 (124, 6) unsegmented-test/wbnSNR-10/gjang/1/kdigits0-1.wav 1 (125, 6) unsegmented-test/wbnSNR-10/gjang/1/kdigits1-1.wav 1 (119, 6) unsegmented-test/wbnSNR-10/gjang/1/kdigits2-1.wav 1 (122, 6) unsegmented-test/wbnSNR-10/gjang/1/kdigits3-1.wav 1 (121, 6) unsegmented-test/wbnSNR-10/gjang/1/kdigits4-1.wav 1 (131, 6) unsegmented-test/wbnSNR-10/gjang/1/kdigits5-1.wav 1 (128, 6) unsegmented-test/wbnSNR-10/gjang/1/kdigits6-1.wav 1 (131, 6) unsegmented-test/wbnSNR-10/gjang/1/kdigits7-1.wav 1 (134, 6) unsegmented-test/wbnSNR-10/gjang/1/kdigits8-1.wav 1 (136, 6) unsegmented-test/wbnSNR-10/gjang/1/kdigits9-1.wav 1 (123, 6) ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 100, 100\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =11.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in ('nbn', 'wbn') :\n",
    "    for j in ('0','10','-10') :\n",
    "        audioclean = addpath('unsegmented-test',f'{i}SNR{j}')\n",
    "        labels = ['gjang']\n",
    "        validation_digits(speechmodels, gmmhmmindexdict, audioclean, labels, 'kdigits', num_trials=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00413f5",
   "metadata": {},
   "source": [
    "## HMM Test 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d2a0f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "testing unsegmented-test/nbnSNR10\n",
      "unsegmented-test/nbnSNR10/gjang/0/kdigits0-0.wav 0 (129, 6) unsegmented-test/nbnSNR10/gjang/0/kdigits1-0.wav 0 (125, 6) unsegmented-test/nbnSNR10/gjang/0/kdigits2-0.wav 0 (119, 6) unsegmented-test/nbnSNR10/gjang/0/kdigits3-0.wav 0 (120, 6) unsegmented-test/nbnSNR10/gjang/0/kdigits4-0.wav 0 (132, 6) unsegmented-test/nbnSNR10/gjang/0/kdigits5-0.wav 0 (135, 6) unsegmented-test/nbnSNR10/gjang/0/kdigits6-0.wav 0 (137, 6) unsegmented-test/nbnSNR10/gjang/0/kdigits7-0.wav 0 (137, 6) unsegmented-test/nbnSNR10/gjang/0/kdigits8-0.wav 0 (136, 6) unsegmented-test/nbnSNR10/gjang/0/kdigits9-0.wav 0 (124, 6) unsegmented-test/nbnSNR10/gjang/1/kdigits0-1.wav 1 (125, 6) unsegmented-test/nbnSNR10/gjang/1/kdigits1-1.wav 1 (119, 6) unsegmented-test/nbnSNR10/gjang/1/kdigits2-1.wav 1 (122, 6) unsegmented-test/nbnSNR10/gjang/1/kdigits3-1.wav 1 (121, 6) unsegmented-test/nbnSNR10/gjang/1/kdigits4-1.wav 1 (131, 6) unsegmented-test/nbnSNR10/gjang/1/kdigits5-1.wav 1 (128, 6) unsegmented-test/nbnSNR10/gjang/1/kdigits6-1.wav 1 (131, 6) unsegmented-test/nbnSNR10/gjang/1/kdigits7-1.wav 1 (134, 6) unsegmented-test/nbnSNR10/gjang/1/kdigits8-1.wav 1 (136, 6) unsegmented-test/nbnSNR10/gjang/1/kdigits9-1.wav 1 (123, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 100, 100\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =10.0\n",
      "\n",
      "-----------------------------------------\n",
      "testing unsegmented-test/nbnSNR0\n",
      "unsegmented-test/nbnSNR0/gjang/0/kdigits0-0.wav 0 (129, 6) unsegmented-test/nbnSNR0/gjang/0/kdigits1-0.wav 0 (125, 6) unsegmented-test/nbnSNR0/gjang/0/kdigits2-0.wav 0 (119, 6) unsegmented-test/nbnSNR0/gjang/0/kdigits3-0.wav 0 (120, 6) unsegmented-test/nbnSNR0/gjang/0/kdigits4-0.wav 0 (132, 6) unsegmented-test/nbnSNR0/gjang/0/kdigits5-0.wav 0 (135, 6) unsegmented-test/nbnSNR0/gjang/0/kdigits6-0.wav 0 (137, 6) unsegmented-test/nbnSNR0/gjang/0/kdigits7-0.wav 0 (137, 6) unsegmented-test/nbnSNR0/gjang/0/kdigits8-0.wav 0 (136, 6) unsegmented-test/nbnSNR0/gjang/0/kdigits9-0.wav 0 (124, 6) unsegmented-test/nbnSNR0/gjang/1/kdigits0-1.wav 1 (125, 6) unsegmented-test/nbnSNR0/gjang/1/kdigits1-1.wav 1 (119, 6) unsegmented-test/nbnSNR0/gjang/1/kdigits2-1.wav 1 (122, 6) unsegmented-test/nbnSNR0/gjang/1/kdigits3-1.wav 1 (121, 6) unsegmented-test/nbnSNR0/gjang/1/kdigits4-1.wav 1 (131, 6) unsegmented-test/nbnSNR0/gjang/1/kdigits5-1.wav 1 (128, 6) unsegmented-test/nbnSNR0/gjang/1/kdigits6-1.wav 1 (131, 6) unsegmented-test/nbnSNR0/gjang/1/kdigits7-1.wav 1 (134, 6) unsegmented-test/nbnSNR0/gjang/1/kdigits8-1.wav 1 (136, 6) unsegmented-test/nbnSNR0/gjang/1/kdigits9-1.wav 1 (123, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 100, 100\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =10.0\n",
      "\n",
      "-----------------------------------------\n",
      "testing unsegmented-test/nbnSNR-10\n",
      "unsegmented-test/nbnSNR-10/gjang/0/kdigits0-0.wav 0 (129, 6) unsegmented-test/nbnSNR-10/gjang/0/kdigits1-0.wav 0 (125, 6) unsegmented-test/nbnSNR-10/gjang/0/kdigits2-0.wav 0 (119, 6) unsegmented-test/nbnSNR-10/gjang/0/kdigits3-0.wav 0 (120, 6) unsegmented-test/nbnSNR-10/gjang/0/kdigits4-0.wav 0 (132, 6) unsegmented-test/nbnSNR-10/gjang/0/kdigits5-0.wav 0 (135, 6) unsegmented-test/nbnSNR-10/gjang/0/kdigits6-0.wav 0 (137, 6) unsegmented-test/nbnSNR-10/gjang/0/kdigits7-0.wav 0 (137, 6) unsegmented-test/nbnSNR-10/gjang/0/kdigits8-0.wav 0 (136, 6) unsegmented-test/nbnSNR-10/gjang/0/kdigits9-0.wav 0 (124, 6) unsegmented-test/nbnSNR-10/gjang/1/kdigits0-1.wav 1 (125, 6) unsegmented-test/nbnSNR-10/gjang/1/kdigits1-1.wav 1 (119, 6) unsegmented-test/nbnSNR-10/gjang/1/kdigits2-1.wav 1 (122, 6) unsegmented-test/nbnSNR-10/gjang/1/kdigits3-1.wav 1 (121, 6) unsegmented-test/nbnSNR-10/gjang/1/kdigits4-1.wav 1 (131, 6) unsegmented-test/nbnSNR-10/gjang/1/kdigits5-1.wav 1 (128, 6) unsegmented-test/nbnSNR-10/gjang/1/kdigits6-1.wav 1 (131, 6) unsegmented-test/nbnSNR-10/gjang/1/kdigits7-1.wav 1 (134, 6) unsegmented-test/nbnSNR-10/gjang/1/kdigits8-1.wav 1 (136, 6) unsegmented-test/nbnSNR-10/gjang/1/kdigits9-1.wav 1 (123, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 100, 100\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =8.0\n",
      "\n",
      "-----------------------------------------\n",
      "testing unsegmented-test/wbnSNR10\n",
      "unsegmented-test/wbnSNR10/gjang/0/kdigits0-0.wav 0 (129, 6) unsegmented-test/wbnSNR10/gjang/0/kdigits1-0.wav 0 (125, 6) unsegmented-test/wbnSNR10/gjang/0/kdigits2-0.wav 0 (119, 6) unsegmented-test/wbnSNR10/gjang/0/kdigits3-0.wav 0 (120, 6) unsegmented-test/wbnSNR10/gjang/0/kdigits4-0.wav 0 (132, 6) unsegmented-test/wbnSNR10/gjang/0/kdigits5-0.wav 0 (135, 6) unsegmented-test/wbnSNR10/gjang/0/kdigits6-0.wav 0 (137, 6) unsegmented-test/wbnSNR10/gjang/0/kdigits7-0.wav 0 (137, 6) unsegmented-test/wbnSNR10/gjang/0/kdigits8-0.wav 0 (136, 6) unsegmented-test/wbnSNR10/gjang/0/kdigits9-0.wav 0 (124, 6) unsegmented-test/wbnSNR10/gjang/1/kdigits0-1.wav 1 (125, 6) unsegmented-test/wbnSNR10/gjang/1/kdigits1-1.wav 1 (119, 6) unsegmented-test/wbnSNR10/gjang/1/kdigits2-1.wav 1 (122, 6) unsegmented-test/wbnSNR10/gjang/1/kdigits3-1.wav 1 (121, 6) unsegmented-test/wbnSNR10/gjang/1/kdigits4-1.wav 1 (131, 6) unsegmented-test/wbnSNR10/gjang/1/kdigits5-1.wav 1 (128, 6) unsegmented-test/wbnSNR10/gjang/1/kdigits6-1.wav 1 (131, 6) unsegmented-test/wbnSNR10/gjang/1/kdigits7-1.wav 1 (134, 6) unsegmented-test/wbnSNR10/gjang/1/kdigits8-1.wav 1 (136, 6) unsegmented-test/wbnSNR10/gjang/1/kdigits9-1.wav 1 (123, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 100, 100\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =11.0\n",
      "\n",
      "-----------------------------------------\n",
      "testing unsegmented-test/wbnSNR0\n",
      "unsegmented-test/wbnSNR0/gjang/0/kdigits0-0.wav 0 (129, 6) unsegmented-test/wbnSNR0/gjang/0/kdigits1-0.wav 0 (125, 6) unsegmented-test/wbnSNR0/gjang/0/kdigits2-0.wav 0 (119, 6) unsegmented-test/wbnSNR0/gjang/0/kdigits3-0.wav 0 (120, 6) unsegmented-test/wbnSNR0/gjang/0/kdigits4-0.wav 0 (132, 6) unsegmented-test/wbnSNR0/gjang/0/kdigits5-0.wav 0 (135, 6) unsegmented-test/wbnSNR0/gjang/0/kdigits6-0.wav 0 (137, 6) unsegmented-test/wbnSNR0/gjang/0/kdigits7-0.wav 0 (137, 6) unsegmented-test/wbnSNR0/gjang/0/kdigits8-0.wav 0 (136, 6) unsegmented-test/wbnSNR0/gjang/0/kdigits9-0.wav 0 (124, 6) unsegmented-test/wbnSNR0/gjang/1/kdigits0-1.wav 1 (125, 6) unsegmented-test/wbnSNR0/gjang/1/kdigits1-1.wav 1 (119, 6) unsegmented-test/wbnSNR0/gjang/1/kdigits2-1.wav 1 (122, 6) unsegmented-test/wbnSNR0/gjang/1/kdigits3-1.wav 1 (121, 6) unsegmented-test/wbnSNR0/gjang/1/kdigits4-1.wav 1 (131, 6) unsegmented-test/wbnSNR0/gjang/1/kdigits5-1.wav 1 (128, 6) unsegmented-test/wbnSNR0/gjang/1/kdigits6-1.wav 1 (131, 6) unsegmented-test/wbnSNR0/gjang/1/kdigits7-1.wav 1 (134, 6) unsegmented-test/wbnSNR0/gjang/1/kdigits8-1.wav 1 (136, 6) unsegmented-test/wbnSNR0/gjang/1/kdigits9-1.wav 1 (123, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 100, 100\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =10.0\n",
      "\n",
      "-----------------------------------------\n",
      "testing unsegmented-test/wbnSNR-10\n",
      "unsegmented-test/wbnSNR-10/gjang/0/kdigits0-0.wav 0 (129, 6) unsegmented-test/wbnSNR-10/gjang/0/kdigits1-0.wav 0 (125, 6) unsegmented-test/wbnSNR-10/gjang/0/kdigits2-0.wav 0 (119, 6) unsegmented-test/wbnSNR-10/gjang/0/kdigits3-0.wav 0 (120, 6) unsegmented-test/wbnSNR-10/gjang/0/kdigits4-0.wav 0 (132, 6) unsegmented-test/wbnSNR-10/gjang/0/kdigits5-0.wav 0 (135, 6) unsegmented-test/wbnSNR-10/gjang/0/kdigits6-0.wav 0 (137, 6) unsegmented-test/wbnSNR-10/gjang/0/kdigits7-0.wav 0 (137, 6) unsegmented-test/wbnSNR-10/gjang/0/kdigits8-0.wav 0 (136, 6) unsegmented-test/wbnSNR-10/gjang/0/kdigits9-0.wav 0 (124, 6) unsegmented-test/wbnSNR-10/gjang/1/kdigits0-1.wav 1 (125, 6) unsegmented-test/wbnSNR-10/gjang/1/kdigits1-1.wav 1 (119, 6) unsegmented-test/wbnSNR-10/gjang/1/kdigits2-1.wav 1 (122, 6) unsegmented-test/wbnSNR-10/gjang/1/kdigits3-1.wav 1 (121, 6) unsegmented-test/wbnSNR-10/gjang/1/kdigits4-1.wav 1 (131, 6) unsegmented-test/wbnSNR-10/gjang/1/kdigits5-1.wav 1 (128, 6) unsegmented-test/wbnSNR-10/gjang/1/kdigits6-1.wav 1 (131, 6) unsegmented-test/wbnSNR-10/gjang/1/kdigits7-1.wav 1 (134, 6) unsegmented-test/wbnSNR-10/gjang/1/kdigits8-1.wav 1 (136, 6) unsegmented-test/wbnSNR-10/gjang/1/kdigits9-1.wav 1 (123, 6) ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 100, 100\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =11.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for outpath in outputpaths:\n",
    "    print('-----------------------------------------')\n",
    "    print('testing', outpath)\n",
    "    validation_digits(speechmodels, gmmhmmindexdict, outpath, labels, 'kdigits', num_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126ef5f6",
   "metadata": {},
   "source": [
    "## Wiener 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f997d037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import wiener\n",
    "\n",
    "output_dir = f\"unsegmented-test/wiener\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "    \n",
    "for i in (\"nbn\", \"wbn\"):\n",
    "    for j in (\"0\", \"10\", \"-10\"):\n",
    "        for k in range(10):\n",
    "            output_dir = f\"unsegmented-test/wiener/{i}SNR{j}/gjang/{k}\"\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bae580a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in (\"nbn\", \"wbn\"):\n",
    "    for j in (\"0\", \"10\", \"-10\"):\n",
    "        for k in range(10):\n",
    "            for s in range(10):\n",
    "                file = f\"kdigits{s}-{k}.wav\"\n",
    "                input_dir = f\"unsegmented-test/{i}SNR{j}/gjang/{k}/{file}\"\n",
    "                signal, Fs = librosa.load(input_dir, sr=None, mono=True)\n",
    "                wiener_filtered = wiener(signal)\n",
    "                \n",
    "                output_dir=f\"unsegmented-test/wiener/{i}SNR{j}/gjang/{k}/{file}\"\n",
    "                wav.writewav(output_dir, Fs, wiener_filtered)\n",
    "                #wiener_filtered.export(output_dir, format=\"wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2cc10f-ae74-4d39-bcdb-5863f58cb805",
   "metadata": {},
   "source": [
    "## End of Project 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965a9905",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
